import streamlit as st
import time
import re
import torch
import plotly.graph_objects as go
from transformers import pipeline
import random
import numpy as np
import lime
from lime.lime_text import LimeTextExplainer
import streamlit.components.v1 as components

# ==========================================
# 1. é é¢å…¨åŸŸè¨­å®š (å¿…é ˆæ”¾åœ¨ç¨‹å¼ç¢¼æœ€ä¸Šæ–¹)
# ==========================================
st.set_page_config(
    page_title="AI æ–‡æœ¬é‘‘è­˜ç³»çµ±",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==========================================
# 2. CSS æ¨£å¼å„ªåŒ– (æå‡å°ˆæ¥­æ„Ÿ)
# ==========================================
st.markdown(r'''
<style>
    /* èª¿æ•´ä¸»å®¹å™¨é ‚éƒ¨é–“è· */
    .block-container {
        padding-top: 2rem;
        padding-bottom: 3rem;
    }
    /* å„ªåŒ–æ–‡å­—è¼¸å…¥æ¡†å­—é«” */
    .stTextArea textarea {
        font-size: 16px;
        line-height: 1.6;
        font-family: 'Inter', sans-serif;
        border-radius: 10px;
    }
    /* è®“æŒ‰éˆ•æ›´é¡¯çœ¼ */
    .stButton button {
        border-radius: 8px;
        font-weight: 600;
        height: 3em;
    }
    /* LIME è§£é‡‹çš„æ¨£å¼ */
    .lime-explanation {
        border: 1px solid #444;
        padding: 15px;
        border-radius: 10px;
        background-color: #262730;
    }
</style>
''', unsafe_allow_html=True)

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸
# ==========================================

@st.cache_resource
def load_detectors():
    """
    è¼‰å…¥å¤šå€‹ Hugging Face æ¨¡å‹ã€‚
    ä½¿ç”¨ @st.cache_resource ç¢ºä¿æ¨¡å‹åªæœƒè¼‰å…¥ä¸€æ¬¡ã€‚
    """
    device = 0 if torch.cuda.is_available() else -1
    model_info = [
        ("ModernBERT Detector", "AICodexLab/answerdotai-ModernBERT-base-ai-detector"),
        ("RoBERTa Detector", "Hello-SimpleAI/chatgpt-detector-roberta")
    ]
    loaded_pipelines = []
    for display_name, model_id in model_info:
        try:
            pipe = pipeline("text-classification", model=model_id, device=device, return_all_scores=True)
            loaded_pipelines.append({"name": display_name, "pipe": pipe, "id": model_id})
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹ '{display_name}' è¼‰å…¥å¤±æ•—: {e}")
    return loaded_pipelines

def clean_text(text: str) -> str:
    """æ¸…ç†è¼¸å…¥æ–‡æœ¬ï¼Œç§»é™¤ä¸å¯è¦‹å­—å…ƒ"""
    text = text.replace("\u200b", " ").replace("\n", " ")
    return re.sub(r"\s+", " ", text).strip()

def create_gauge_chart(score, title="ç¶œåˆè©•åˆ†"):
    bar_color = "#10B981" if score > 50 else "#EF4444"
    fig = go.Figure(go.Indicator(
        mode="gauge+number", value=score,
        domain={'x': [0, 1], 'y': [0, 1]},
        title={'text': title, 'font': {'size': 20, 'color': "gray"}},
        number={'suffix': "%", 'font': {'size': 40}},
        gauge={
            'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': "darkblue"},
            'bar': {'color': bar_color}, 'bgcolor': "white", 'borderwidth': 2, 'bordercolor': "#E5E7EB",
            'steps': [{'range': [0, 50], 'color': 'rgba(239, 68, 68, 0.1)'}, {'range': [50, 100], 'color': 'rgba(16, 185, 129, 0.1)'}],
            'threshold': {'line': {'color': "black", 'width': 3}, 'thickness': 0.75, 'value': score}
        }
    ))
    fig.update_layout(height=250, margin=dict(l=20, r=20, t=40, b=20), paper_bgcolor='rgba(0,0,0,0)', font={'family': "Arial"})
    return fig

def get_verdict(score):
    if score > 80: return "âœ… é«˜æ©Ÿç‡ç‚ºäººé¡æ’°å¯«"
    elif score > 50: return "âš ï¸ å¯èƒ½ç‚ºæ··åˆå…§å®¹ / æ¨¡ç¨œå…©å¯"
    else: return "ğŸ¤– é«˜æ©Ÿç‡ç”± AI ç”Ÿæˆ"

# ==========================================
# 4. LIME è§£é‡‹å™¨ç›¸é—œå‡½æ•¸
# ==========================================

def get_lime_predictor(pipe, model_name):
    def predictor(texts):
        predictions = pipe(texts, truncation=True, max_length=512)
        probs = []
        for text_preds in predictions:
            prob_map = {p['label']: p['score'] for p in text_preds}
            human_prob, ai_prob = 0.0, 0.0
            if model_name == "ModernBERT Detector":
                human_prob, ai_prob = prob_map.get("LABEL_0", 0.0), prob_map.get("LABEL_1", 0.0)
            elif model_name == "RoBERTa Detector":
                human_prob = prob_map.get("Human", prob_map.get("Real", 0.0))
                ai_prob = prob_map.get("ChatGPT", prob_map.get("Fake", 0.0))
            probs.append([ai_prob, human_prob])
        return np.array(probs)
    return predictor

# ==========================================
# 5. ä¸»ç¨‹å¼é‚è¼¯
# ==========================================
def main():
    # --- åˆå§‹åŒ– Session State ---
    if 'text_content' not in st.session_state: st.session_state.text_content = ""
    if 'analysis_results' not in st.session_state: st.session_state.analysis_results = None
    if 'lime_html' not in st.session_state: st.session_state.lime_html = None
    if 'available_indices' not in st.session_state: 
        sample_texts = { "AI": [...], "Human": [...] }
        st.session_state.all_samples = sum(sample_texts.values(), [])
        st.session_state.available_indices = list(range(len(st.session_state.all_samples)))

    # --- å´é‚Šæ¬„ ---
    with st.sidebar:
        st.header("ğŸ›¡ï¸ AI Sentinel"); st.caption("ç‰ˆæœ¬ v4.1 | LIME è§£é‡‹æ•´åˆ")
        st.info("**ğŸ“Š åˆ¤è®€æŒ‡å—ï¼š**\næœ¬å·¥å…·ä½¿ç”¨é›™æ¨¡å‹åˆ†æä¾†æå‡æº–ç¢ºåº¦ã€‚")
        st.markdown("### ä½¿ç”¨æ¨¡å‹\n- **ModernBERT**: æ–°ä¸€ä»£é«˜æ•ˆèƒ½æ¶æ§‹ã€‚\n- **RoBERTa**: ç¶“å…¸ä¸”ç©©å®šçš„åµæ¸¬æ¨¡å‹ã€‚")
        st.success("**æ–°å¢åŠŸèƒ½ï¼š**\nå ±å‘Šåº•éƒ¨æ–°å¢ LIME å¯è¦–åŒ–è§£é‡‹ï¼Œæ¨™ç¤ºå½±éŸ¿åˆ¤æ–·çš„é—œéµè©å½™ã€‚")
        st.markdown("**ğŸ’¡ æ³¨æ„ï¼š**\nçµæœåƒ…ä¾›åƒè€ƒï¼Œä¸æ‡‰ä½œç‚ºçµ•å°ä¾æ“šã€‚"); st.caption("Designed for HW5")

    # --- è¼‰å…¥æ¨¡å‹ ---
    active_pipelines = load_detectors()

    # --- ä¸»ä»‹é¢ ---
    st.title("ğŸ•µï¸â€â™‚ï¸ å°ˆæ¥­ç´š AI å…§å®¹æª¢æ¸¬å„€")
    st.markdown("#### é€éé›™æ¨¡å‹äº¤å‰é©—è­‰èˆ‡ LIME è§£é‡‹ï¼Œæ·±å…¥äº†è§£ AI çš„åˆ¤æ–·ä¾æ“š")
    st.markdown("---")
    
    col1, col2 = st.columns([1.2, 1], gap="large")

    with col1:
        st.subheader("ğŸ“ è¼¸å…¥å¾…æ¸¬æ–‡æœ¬")
        btn_cols = st.columns([1, 1])
        if btn_cols[0].button("éš¨æ©Ÿç¯„ä¾‹", key="random_sample"):
            if not st.session_state.available_indices:
                st.session_state.available_indices = list(range(len(st.session_state.all_samples)))
                st.toast("æ‰€æœ‰ç¯„ä¾‹å·²é¡¯ç¤ºå®Œç•¢ï¼Œåˆ—è¡¨å·²é‡ç½®ã€‚")
            random_index = random.choice(st.session_state.available_indices)
            st.session_state.text_content = st.session_state.all_samples[random_index]
            st.session_state.available_indices.remove(random_index)
            st.session_state.analysis_results, st.session_state.lime_html = None, None # æ¸…é™¤èˆŠçµæœ
            st.rerun()

        if btn_cols[1].button("ğŸ—‘ï¸ æ¸…ç©º", key="clear"):
            st.session_state.text_content, st.session_state.analysis_results, st.session_state.lime_html = "", None, None
            st.rerun()

        input_text = st.text_area("è«‹åœ¨æ­¤è²¼ä¸Šæ–‡ç« å…§å®¹ (å»ºè­°è‹±æ–‡):", value=st.session_state.text_content, height=350, key="text_area_input")
        word_count = len(input_text.split())
        st.caption(f"ç›®å‰å­—æ•¸: {word_count} words")
        analyze_btn = st.button("ğŸ” é–‹å§‹äº¤å‰åˆ†æ", type="primary", use_container_width=True, disabled=(not active_pipelines))

    # --- åˆ†ææŒ‰éˆ•é‚è¼¯ ---
    if analyze_btn:
        st.session_state.text_content = st.session_state.text_area_input # æ›´æ–° state
        if not st.session_state.text_content.strip() or len(st.session_state.text_content.split()) < 3:
            st.session_state.analysis_results, st.session_state.lime_html = None, None
            with col2: st.warning("âš ï¸ è«‹è¼¸å…¥è‡³å°‘ 3 å€‹å–®å­—çš„æœ‰æ•ˆæ–‡å­—å…§å®¹ï¼")
        else:
            with st.spinner("æ­£åœ¨åŸ·è¡Œäº¤å‰é©—è­‰èˆ‡ LIME è§£é‡‹..."):
                safe_text = clean_text(st.session_state.text_content)
                # 1. åˆ†æ•¸è¨ˆç®—
                results_detail = []
                for item in active_pipelines:
                    pipe, name = item['pipe'], item['name']
                    prediction_list = pipe(safe_text)[0]
                    prob_map = {p['label']: p['score'] for p in prediction_list}
                    human_prob = 0.0
                    if name == "ModernBERT Detector": human_prob = prob_map.get("LABEL_0", 0.0)
                    elif name == "RoBERTa Detector": human_prob = prob_map.get("Human", prob_map.get("Real", 0.0))
                    results_detail.append({ "name": name, "prob": human_prob })
                
                st.session_state.analysis_results = {
                    'avg_score': (sum(r['prob'] for r in results_detail) / len(results_detail)) * 100,
                    'details': results_detail
                }
                
                # 2. LIME è§£é‡‹
                model_to_explain = active_pipelines[0]
                explainer = LimeTextExplainer(class_names=["AI", "Human"])
                lime_predictor = get_lime_predictor(model_to_explain['pipe'], model_to_explain['name'])
                explanation = explainer.explain_instance(safe_text, lime_predictor, num_features=15, labels=(0, 1))
                st.session_state.lime_html = explanation.as_html(labels=(1,0))
                st.session_state.lime_model_name = model_to_explain['name']
            st.rerun()

    # --- çµæœé¡¯ç¤ºå€ ---
    with col2:
        st.subheader("ğŸ“Š ç¶œåˆåˆ†æå ±å‘Š")
        if not active_pipelines:
            st.error("âŒ ç„¡æ³•è¼‰å…¥ä»»ä½• AI æ¨¡å‹ã€‚")
        elif st.session_state.analysis_results:
            results = st.session_state.analysis_results
            st.plotly_chart(create_gauge_chart(results['avg_score']), use_container_width=True)
            st.markdown(f"<h3 style='text-align: center; color: #FFF;'>{get_verdict(results['avg_score'])}</h3>", unsafe_allow_html=True)
            st.markdown("---")
            st.write("##### ğŸ”¬ é›™æ¨¡å‹äº¤å‰æ¯”å°çµæœï¼š")
            for res in results['details']:
                st.markdown(f"**{res['name']}**"); st.progress(res['prob'])
                st.caption(f"äººé¡æ©Ÿç‡: {res['prob']:.2%} | {get_verdict(res['prob']*100)}")
            
            # é¡¯ç¤º LIME è§£é‡‹
            if st.session_state.lime_html:
                st.markdown("---")
                st.subheader("ğŸ’¡ æ¨¡å‹åˆ¤æ–·ä¾æ“š (LIME)")
                st.info(f"ä¸‹æ–¹é¡¯ç¤º **{st.session_state.lime_model_name}** çš„åˆ¤æ–·ä¾æ“šã€‚ç¶ è‰²ç‚º **Human** å‚¾å‘ï¼Œç´…è‰²ç‚º **AI** å‚¾å‘ã€‚")
                components.html(st.session_state.lime_html, height=400, scrolling=True)
        else:
            st.info("ğŸ‘ˆ è«‹åœ¨å·¦å´è¼¸å…¥æ–‡ç« ï¼Œä¸¦é»æ“ŠæŒ‰éˆ•é–‹å§‹åˆ†æã€‚")

if __name__ == "__main__":
    main()