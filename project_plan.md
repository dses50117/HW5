# Project Plan: AI Content Detector (CRISP-DM Framework)

This document outlines the plan and development process for the AI Content Detector application, structured according to the Cross-Industry Standard Process for Data Mining (CRISP-DM).

---

## 1. Business Understanding

### Objective
The primary business objective is to create a user-friendly web application that can determine whether a piece of text was written by a human or generated by an AI. The initial goal was to build a functional prototype that demonstrates the user interface and core functionality.

### Project Goals
*   **Functionality:** The application must provide a text area for user input, a button to initiate analysis, and a clear results display.
*   **User Experience:** The interface should be simple, intuitive, and allow users to either paste their own text or load a sample text.
*   **Success Criteria:** A successful project is a deployed Streamlit application that meets the UI/UX goals and correctly simulates the analysis process, laying the groundwork for a future version with a real AI model.

---

## 2. Data Understanding

This phase is about sourcing and exploring the data. For this project, the approach differed between the prototype and what a real project would require.

### Prototype Approach
For the initial prototype, no external data was collected.
*   **Sample Data:** A predefined list of five text snippets was hardcoded directly into the `app.py` script. This was used for the "Use Sample" feature.
    ```python
    sample_texts = [
        "The quick brown fox jumps over the lazy dog...",
        "Artificial intelligence (AI) is intelligence...",
        # ... etc.
    ]
    ```
*   **Exploration:** There was no data exploration, as the data was purely for demonstration.

### Real Project Approach
For a real AI detector, this would be a critical phase:
1.  **Data Sourcing:** Collect a large, diverse dataset containing two categories of text:
    *   **Human-Written Text:** From sources like Wikipedia, news articles, books, and academic papers.
    *   **AI-Generated Text:** From various large language models (e.g., GPT-3, GPT-4, Llama) on a wide range of topics.
2.  **Data Labeling:** Each document in the dataset would be strictly labeled as either `human` or `ai`.
3.  **Exploratory Data Analysis (EDA):** Analyze the texts to find patterns that differentiate the two classes. This involves examining:
    *   **Statistical Properties:** Sentence length, word choice, vocabulary diversity (lexical density).
    *   **Grammatical Structure:** Consistency and complexity of grammar.
    *   **"Burstiness":** Human writing often has uneven, or "bursty," sentence lengths, while AI text can be more uniform.
    *   **Visualization:** Use histograms, word clouds, and other plots to visualize these differences.

---

## 3. Data Preparation

### Prototype Approach
This phase was not applicable to the prototype, as we used no real data that required cleaning or preparation.

### Real Project Approach
1.  **Cleaning:** Pre-process the collected text data by removing irrelevant content (like HTML tags), standardizing punctuation, and correcting any formatting issues.
2.  **Tokenization:** Break down the text into smaller units (words or sub-words).
3.  **Splitting:** Divide the prepared dataset into three sets:
    *   **Training Set:** To train the AI model.
    *   **Validation Set:** To tune the model's parameters during training.
    *   **Test Set:** To evaluate the final performance of the model on unseen data.

---

## 4. Modeling

### Prototype Approach
The "modeling" phase was simulated to mimic the behavior of a real AI model.
*   **Placeholder Model:** A simple random number generator was used to create a "human content" score. This approach allowed us to build and test the user interface without a trained model.
    ```python
    # This line simulates the output of a real model
    human_score = random.randint(50, 100)
    ```

### Real Project Approach
1.  **Model Selection:** Choose an appropriate machine learning model architecture. Transformer-based models (like BERT or RoBERTa) are well-suited for this task. A model could be fine-tuned from a pre-trained version, perhaps from a repository like Hugging Face.
2.  **Training:** Train the selected model on the prepared training dataset. The model learns the patterns and features identified during the data understanding phase to distinguish between human and AI text.
3.  **Tuning:** Use the validation set to fine-tune the model's hyperparameters for optimal performance.

---

## 5. Evaluation

### Prototype Approach
Evaluation focused on the application's functionality and user experience.
*   **UI Testing:** We iteratively tested the app's layout and features based on user feedback (e.g., changing from a checkbox to buttons, moving the buttons below the text area).
*   **Bug Fixing:** We debugged issues like the `StreamlitAPIException` that arose during development to ensure the app ran smoothly.

### Real Project Approach
1.  **Performance Metrics:** The trained model would be rigorously evaluated on the unseen test set using standard classification metrics:
    *   **Accuracy:** Overall percentage of correct predictions.
    *   **Precision:** Of the texts predicted as AI, how many actually were?
    *   **Recall:** Of all the actual AI texts, how many did the model find?
    *   **F1-Score:** The harmonic mean of precision and recall.
2.  **Error Analysis:** Manually review cases where the model made incorrect predictions to understand its weaknesses and identify areas for improvement.

---

## 6. Deployment

This phase involves making the application available to end-users. This was the primary focus of our work.

### Prototype Deployment
1.  **Application Development:** The entire user interface was built in the `app.py` script using the Streamlit library.
2.  **State Management:** `st.session_state` was used to manage the text content, ensuring a smooth user experience as the user interacted with buttons and the text area.
3.  **Iterative Refinement:** The UI was built and refined over several iterations based on direct user requests.
4.  **Running the App:** The application is "deployed" locally by running the following command in the terminal:
    ```bash
    streamlit run app.py
    ```

### Real Project Approach
1.  **Model Integration:** The trained and evaluated AI model would be saved and integrated into the Streamlit application, replacing the `random.randint()` placeholder.
2.  **Hosting:** The application would be deployed to a cloud service (like Streamlit Community Cloud, AWS, or Heroku) to make it accessible to users via a public URL.
3.  **Monitoring:** A system would be put in place to monitor the model's performance in the real world and collect new data for future retraining.